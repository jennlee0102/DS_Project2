{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F8n73FikZrl",
        "outputId": "7f05bc83-023c-4d51-8fc2-d4f35d8599b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCusrJRQJIp1"
      },
      "source": [
        "##Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_77bXI-O7PA",
        "outputId": "da1226b5-40d3-4340-b4ac-2ca8f254a5ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69269, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/project/3rd project_recommender_system/data_preparation/final.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syhLZH57L9F8",
        "outputId": "ddb47c98-32f4-4a79-a8c1-0bed637c1638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 69269 entries, 0 to 69268\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   business_id    69269 non-null  object \n",
            " 1   name           69269 non-null  object \n",
            " 2   state          69269 non-null  object \n",
            " 3   stars_x        69269 non-null  float64\n",
            " 4   is_open        69269 non-null  int64  \n",
            " 5   attributes     69269 non-null  object \n",
            " 6   categories     69269 non-null  object \n",
            " 7   user_id        69269 non-null  object \n",
            " 8   useful         69269 non-null  int64  \n",
            " 9   funny          69269 non-null  int64  \n",
            " 10  cool           69269 non-null  int64  \n",
            " 11  text           69269 non-null  object \n",
            " 12  review_tokens  69269 non-null  object \n",
            "dtypes: float64(1), int64(4), object(8)\n",
            "memory usage: 6.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFO8s10PMZ2F",
        "outputId": "08a885af-4f42-45e9-e548-4edb46adae61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51609, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = df.drop(index=df[df['is_open'] == 0].index)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection"
      ],
      "metadata": {
        "id": "3DI4tI96bcvj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLn_G8OF6BiJ"
      },
      "source": [
        "model selection not for deep learning\n",
        "\n",
        "**Collaborative Filtering** recommends items based on similar users' preferences.<br>\n",
        "**Content-based Filtering** recommends items based on the attributes of the items.<br>\n",
        "**Hybrid Filtering** combines both Collaborative and Content-based filtering to improve the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnrTw0Wryx2B"
      },
      "source": [
        "- I need to first create the necessary data structures to store the required information.\n",
        "\n",
        "1. **Collaborative Filtering**: \n",
        "I will use the Surprise library to implement collaborative filtering. Surprise is a Python library for building and analyzing recommender systems that deal with explicit rating data.\n",
        "\n",
        "2. **Content-based Filtering**:\n",
        "I will use the TF-IDF matrix that I created earlier. I can compute the pairwise cosine similarity between all the items using the cosine_similarity function from scikit-learn.\n",
        "\n",
        "3. **Hybrid Filtering**:\n",
        "Once I have the necessary data structures, I can use them to build a hybrid recommendation system. The basic idea behind hybrid filtering is to combine the predictions from collaborative and content-based filtering models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xsQLX3e2xuKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a107d4f6-a71d-46c0-ecc3-1215427abef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.9/dist-packages (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-surprise) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lNPKYOwNxufV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a494efda-18d1-4e47-86b4-f7410ae48ea8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f6b1c31b190>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Collaborative Filtering\n",
        "import surprise\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Define a reader to read in the rating data\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load the rating data into a Surprise dataset\n",
        "data = Dataset.load_from_df(df[['user_id', 'business_id', 'stars_x']], reader)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# Define a collaborative filtering model using SVD\n",
        "model_collab = SVD()\n",
        "\n",
        "# Train the model on the rating data\n",
        "model_collab.fit(trainset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frQNrQjeyUb7"
      },
      "source": [
        "SVD can be used to perform Latent Semantic Analysis (LSA) in NLP by transforming a document-term matrix into a document-concept matrix, where the concepts are derived from the singular values and vectors of the document-term matrix. This can be useful for text classification, information retrieval, and topic modeling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join the preprocessed text into a single string\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Define the vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "# ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000\n",
        "\n",
        "# Fit and transform the reviews\n",
        "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
        "\n",
        "# Content-based Filtering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Convert tfidf_matrix to a sparse matrix\n",
        "tfidf_sparse = csr_matrix(tfidf_matrix)\n",
        "\n",
        "# Compute the cosine similarity using the sparse matrix\n",
        "cosine_sim = cosine_similarity(tfidf_sparse)\n",
        "# Compute the pairwise cosine similarity between all the items\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "ncGnkxekGec3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VaviWnK121k"
      },
      "source": [
        "Here, I will use a simple linear combination of the two models' predictions, where the weight given to each model is determined by a parameter alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8h5TnGrfykf0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Hybrid Filtering\n",
        "def hybrid_recommendations(user_id, business_id):\n",
        "    \"\"\"\n",
        "    Get hybrid recommendations for a user and a business.\n",
        "    \n",
        "    Parameters:\n",
        "        - user_id: ID of the user for whom to make recommendations\n",
        "        - business_id: ID of the business for which to make recommendations\n",
        "        - alpha: Weight given to the collaborative filtering model (default: 0.5)\n",
        "    \n",
        "    Returns:\n",
        "        - A list of top 5 recommended business names and their corresponding predicted ratings\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the indices of the user and business in the data matrix\n",
        "    user_idx = np.where(df['user_id'] == user_id)[0][0]\n",
        "    business_idx = np.where(df['business_id'] == business_id)[0][0]\n",
        "    \n",
        "    # Compute the collaborative filtering prediction\n",
        "    collab_pred = model_collab.predict(user_id, business_id).est\n",
        "    \n",
        "    # Compute the content-based filtering predictions\n",
        "    # content_preds = cosine_sim[business_idx]\n",
        "    \n",
        "    # Combine the predictions from both models\n",
        "    hybrid_pred = collab_pred\n",
        "    # hybrid_pred = (1 - alpha) * collab_pred + alpha * content_preds\n",
        "    \n",
        "    # Get the top 5 recommended business names and their predicted ratings\n",
        "    hybrid_preds = list(enumerate(hybrid_pred))\n",
        "    hybrid_preds = sorted(hybrid_preds, key=lambda x: x[1], reverse=True)\n",
        "    top_business_indices = [x[0] for x in hybrid_preds[:5]]\n",
        "    top_business_names = df.iloc[top_business_indices]['name'].tolist()\n",
        "    # top_business_ratings = [x[1] for x in hybrid_preds[:5]]\n",
        "    top_business_ratings = hybrid_pred[top_business_indices].tolist()\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the user ID and business ID\n",
        "user_id = '0q2W3-ieBUJWD5TTLKi3Ug'\n",
        "business_id = 'MTSW4McQd7CbVtyjqoe9mw'\n",
        "\n",
        "# Call the hybrid_recommendations() function with alpha=0.5\n",
        "recommendations = hybrid_recommendations(user_id, business_id)\n",
        "\n",
        "# Print the recommendations\n",
        "print(recommendations)"
      ],
      "metadata": {
        "id": "OfEVnKtUEKSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hgle7VEj08v"
      },
      "source": [
        "### Challenge\n",
        "\n",
        "I want to make hybrid recommendations. However, the calculation of the cosine similarity matrix using cosine_similarity is causing the RAM to crash, due to the large size of the matrix.\n",
        "\n",
        "So, the used model here is Collaborative Filtering using SVD without contents-based Filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Others"
      ],
      "metadata": {
        "id": "rLn42JC0bWI8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuiZa-N3tgrE"
      },
      "source": [
        "### converting reviews into numerical representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf3SVijTA93S"
      },
      "source": [
        "Count-Based Representation\n",
        "\n",
        "> TF-IDF Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns8ui1C8rwLb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Join the preprocessed text into a single string\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# # Define the vectorizer\n",
        "# tfidf = TfidfVectorizer()\n",
        "# # ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000\n",
        "# # Fit and transform the reviews\n",
        "# tfidf_matrix = tfidf.fit_transform(df['review_text'])\n",
        "\n",
        "# # # Create DTM\n",
        "# # dtm_tfidf = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# # # Print the shape of the TF-IDF matrix\n",
        "# # print(tfidf_matrix.shape)\n",
        "# # display(dtm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thf4gaE3jBb-"
      },
      "source": [
        "I've added some additional parameters to the TfidfVectorizer:\n",
        "\n",
        "- ngram_range=(1,2): This specifies that the vectorizer should consider both unigrams and bigrams when creating features.\n",
        "- max_df=0.75: This specifies that words should be excluded from the vocabulary if they appear in more than 75% of the documents.\n",
        "- min_df=5: This specifies that words should be excluded from the vocabulary if they appear in fewer than 5 documents.\n",
        "- max_features=5000: This specifies that the vectorizer should consider at most 5000 features (i.e., the 5000 most frequent words in the corpus)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bailzEacjs0F"
      },
      "source": [
        "how to split the data into training and test sets, train a recommender system using TF-IDF, generate recommendations for each user in the test set, and calculate the MAP score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fhfXCc8bBGU"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "# from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# # df['review_text'] = df['review_tokens'].apply(clean_text)\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# vec = CountVectorizer(ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n",
        "\n",
        "# # Fit and transform the reviews\n",
        "# vec_train = vec.fit_transform(train_df['review_text'])\n",
        "# print(vec_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg-PKDu7rCax"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# Join the preprocessed text into a single string\n",
        "# df['review_text'] = df['review_tokens'].apply(clean_text)\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Define the vectorizer\n",
        "# vec = CountVectorizer(ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n",
        "\n",
        "# # Fit and transform the reviews\n",
        "# vec_matrix_train = vec.fit_transform(train_df['review_tokens'])\n",
        "\n",
        "# # Initialize cosine similarity matrix\n",
        "# cosine_sim = cosine_similarity(vec_matrix_train, vec_matrix_train)\n",
        "\n",
        "# # Define function to get top recommendations for each user\n",
        "# def recommendations(user_id, cosine_sim, df, top_n=5):\n",
        "#     # Get index of user_id in df\n",
        "#     user_index = df[df['user_id'] == user_id].index[0]\n",
        "\n",
        "#     # Get cosine similarity scores for user_index\n",
        "#     sim_scores = list(enumerate(cosine_sim[user_index]))\n",
        "\n",
        "#     # Sort the list of sim_scores in descending order\n",
        "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#     # Get indices of top_n similar users\n",
        "#     top_similar_users = [i[0] for i in sim_scores[1:top_n+1]]\n",
        "\n",
        "#     # Get the restaurant recommendations for the top_n similar users\n",
        "#     recommended_restaurants = df.iloc[top_similar_users][['name', 'categories', 'stars_x']]\n",
        "\n",
        "#     return recommended_restaurants\n",
        "\n",
        "# user_id = 'fJ3iKa2YmdNMOOy4L_R9kQ'\n",
        "# top_n = 5\n",
        "# recommended_restaurants = recommendations(user_id, cosine_sim, df, top_n)\n",
        "# recommended_restaurants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEQHlmgocgZT"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Create TF-IDF vectorizer\n",
        "# tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.75, min_df=5, max_features=5000)\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# tfidf_matrix_train = tfidf.fit_transform(train_df['text'])\n",
        "\n",
        "# # Initialize cosine similarity matrix\n",
        "# cosine_sim = cosine_similarity(tfidf_matrix_train, tfidf_matrix_train)\n",
        "\n",
        "# # Get a list of restaurants reviewed by a given user\n",
        "# def get_user_reviews(user_id, df):\n",
        "#     return df[df['user_id'] == user_id]['business_id'].tolist()\n",
        "\n",
        "# # Find similar restaurants for a given restaurant\n",
        "# def get_similar_restaurants(restaurant_id, tfidf_matrix, df):\n",
        "#     restaurant_idx = df[df['business_id'] == restaurant_id].index[0]\n",
        "#     similarity_scores = cosine_similarity(tfidf_matrix[restaurant_idx], tfidf_matrix)\n",
        "#     similar_restaurants = list(enumerate(similarity_scores[0]))\n",
        "#     similar_restaurants = sorted(similar_restaurants, key=lambda x: x[1], reverse=True)\n",
        "#     return similar_restaurants[1:]\n",
        "\n",
        "# # Make recommendations for a given user\n",
        "# def recommend_restaurants(user_id, tfidf_matrix, df, top_n=5):\n",
        "#     user_reviews = get_user_reviews(user_id, df)\n",
        "#     restaurant_scores = {}\n",
        "#     for restaurant_id in user_reviews:\n",
        "#         similar_restaurants = get_similar_restaurants(restaurant_id, tfidf_matrix, df)\n",
        "#         for i, (idx, score) in enumerate(similar_restaurants):\n",
        "#             if idx in restaurant_scores:\n",
        "#                 restaurant_scores[idx] += score * (0.95 ** i)\n",
        "#             else:\n",
        "#                 restaurant_scores[idx] = score * (0.95 ** i)\n",
        "#     restaurant_scores = sorted(restaurant_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "#     recommended_restaurants = [df.iloc[idx]['business_id'] for idx, score in restaurant_scores[:top_n]]\n",
        "#     return recommended_restaurants\n",
        "\n",
        "# user_id = 'fJ3iKa2YmdNMOOy4L_R9kQ'\n",
        "# top_n = 5\n",
        "# recommended_restaurants = recommend_restaurants(user_id, tfidf_matrix, df, top_n)\n",
        "# recommended_restaurants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olrin9aMlgv4"
      },
      "outputs": [],
      "source": [
        "# # Define function to calculate Average Precision (AP) for a given user\n",
        "# def apk(actual, predicted, k=10):\n",
        "#     if len(predicted) > k:\n",
        "#         predicted = predicted[:k]\n",
        "\n",
        "#     score = 0.0\n",
        "#     num_hits = 0.0\n",
        "\n",
        "#     for i, p in enumerate(predicted):\n",
        "#         if p in actual and p not in predicted[:i]:\n",
        "#             num_hits += 1.0\n",
        "#             score += num_hits / (i+1.0)\n",
        "\n",
        "#     if not actual:\n",
        "#         return 0.0\n",
        "\n",
        "#     return score / min(len(actual), k)\n",
        "\n",
        "# # Get unique user_ids in test_df\n",
        "# users_test = test_df['user_id'].unique()\n",
        "\n",
        "# # Initialize list to store AP scores for each user\n",
        "# ap_scores = []\n",
        "\n",
        "# # Generate recommendations and calculate AP for each user in test_df\n",
        "# for user in users_test:\n",
        "#     # Get recommendations for user\n",
        "#     recommendations = get_recommendations(user, cosine_sim, train_df)\n",
        "\n",
        "#     # Get actual restaurants rated by user in test_df\n",
        "#     actual = test_df[test_df['user_id'] == user]['name'].tolist()\n",
        "\n",
        "#     # Calculate AP for user\n",
        "#     ap = apk(actual, recommendations['name'].tolist())\n",
        "\n",
        "#     # Append AP score to ap_scores\n",
        "#     ap_scores.append(ap)\n",
        "\n",
        "# # Calculate MAP score\n",
        "# map_score = sum(ap_scores) / len(ap_scores)\n",
        "\n",
        "# print(\"MAP Score: \", map_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypj1rpVuslJ-"
      },
      "source": [
        "The main difference between the two is that CountVectorizer simply counts the number of times each word appears in a document, while TfidfVectorizer takes into account the frequency of the word in the entire corpus of documents.\n",
        "\n",
        "TfidfVectorizer gives more weight to words that are more important or informative for a given document, while CountVectorizer treats all words equally.\n",
        "\n",
        "Refer to https://arxiv.org/pdf/2004.13851.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORYS6rocjzz"
      },
      "outputs": [],
      "source": [
        "# user_id = 'your_user_id_here'\n",
        "# top_n = 5\n",
        "# recommended_restaurants = recommend_restaurants(user_id, tfidf_matrix, df, top_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCul31oydmjY"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def calculate_apk(actual, predicted, k=10):\n",
        "#     \"\"\"\n",
        "#     Calculates the Average Precision at k for a single user.\n",
        "    \n",
        "#     Args:\n",
        "#     actual (list): A list of the actual restaurant IDs (as strings).\n",
        "#     predicted (list): A list of the predicted restaurant IDs (as strings).\n",
        "#     k (int): The number of recommendations to consider.\n",
        "    \n",
        "#     Returns:\n",
        "#     apk (float): The Average Precision at k for the user.\n",
        "#     \"\"\"\n",
        "#     if len(predicted) > k:\n",
        "#         predicted = predicted[:k]\n",
        "    \n",
        "#     score = 0.0\n",
        "#     num_hits = 0.0\n",
        "    \n",
        "#     for i, p in enumerate(predicted):\n",
        "#         if p in actual and p not in predicted[:i]:\n",
        "#             num_hits += 1.0\n",
        "#             score += num_hits / (i+1.0)\n",
        "            \n",
        "#     if not actual:\n",
        "#         return 0.0\n",
        "    \n",
        "#     return score / min(len(actual), k)\n",
        "\n",
        "# def calculate_map(actual_dict, predicted_dict, k=10):\n",
        "#     \"\"\"\n",
        "#     Calculates the Mean Average Precision for a set of recommendations.\n",
        "    \n",
        "#     Args:\n",
        "#     actual_dict (dict): A dictionary mapping user IDs to lists of actual restaurant IDs.\n",
        "#     predicted_dict (dict): A dictionary mapping user IDs to lists of predicted restaurant IDs.\n",
        "#     k (int): The number of recommendations to consider.\n",
        "    \n",
        "#     Returns:\n",
        "#     map (float): The Mean Average Precision for the recommendations.\n",
        "#     \"\"\"\n",
        "#     apks = []\n",
        "#     for user_id in actual_dict.keys():\n",
        "#         actual = actual_dict[user_id]\n",
        "#         predicted = predicted_dict.get(user_id, [])\n",
        "#         apk = calculate_apk(actual, predicted, k=k)\n",
        "#         apks.append(apk)\n",
        "        \n",
        "#     return np.mean(apks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8-qNaUTMOxZ"
      },
      "source": [
        "two classes: \"liked by the user\" & \"not liked by the user\"<br/>\n",
        "-->Binary classification prob."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAtpsfOCBW6U"
      },
      "source": [
        "Distributed-Based Representation\n",
        "\n",
        "\n",
        "> Embedding Matrix: Pretrained word embedding GloVe\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWYHPt-dHUzb"
      },
      "source": [
        "This code uses GloVe pre-trained word embeddings to create an embedding matrix, and builds a deep learning model using LSTM layers to predict restaurant ratings. It then trains the model on the training data, evaluates the model on the test data, and makes restaurant recommendations based on the user's preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIij-LaRKEOO",
        "outputId": "33f8105b-95a5-4a6b-9e1f-534a15a72f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# # Download and load the pre-trained GloVe word embeddings\n",
        "# word_vectors = api.load(\"glove-wiki-gigaword-100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntyvv9rx8TGT"
      },
      "source": [
        "Gensim is an open-source Python library designed to process and analyze large-scale collections of text data. It provides implementations of several state-of-the-art algorithms for natural language processing (NLP), including topic modeling, document similarity analysis, and word embedding.\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) is a popular unsupervised algorithm for generating word embeddings, which are dense vector representations of words that capture their semantic and syntactic meaning. The GloVe algorithm is typically used to train word embeddings on large text corpora.\n",
        "\n",
        "While GloVe itself is not directly integrated into Gensim, Gensim provides an interface for loading pre-trained GloVe embeddings and using them for downstream NLP tasks. This makes it easy to incorporate GloVe embeddings into your own NLP models built with Gensim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpCc4ye_pHEl"
      },
      "source": [
        "## Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-SfBRGeskxV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8JkoxnwCn9d"
      },
      "outputs": [],
      "source": [
        "# import json, os\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from time import sleep, time\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import pickle\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# import psycopg2\n",
        "# import gensim\n",
        "# import gensim.downloader as api\n",
        "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "# import re\n",
        "# from collections import namedtuple\n",
        "\n",
        "# doc_vectorizer = Doc2Vec(dm=1, vector_size=300, window=5, alpha=0.025, min_alpha=0.025, seed=1111)\n",
        "\n",
        "# # Assume that the tokenized reviews are stored as a list of strings in the 'review_tokens' column of the DataFrame\n",
        "# # Convert the tokenized reviews to sentences\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join([word.replace(\"'\", \"\").replace(\",\", \"\") for word in x]))\n",
        "# df['review_text'] = df['review_tokens'].apply(lambda x: ' '.join([' '.join(tokens) for tokens in df['review_text']]))\n",
        "\n",
        "# # Print the 'review_text' column to verify the result\n",
        "# print(df['review_text'])\n",
        "\n",
        "# # # Store the pre-tokenized reviews in the 'review_tokens' column of the DataFrame\n",
        "# # df['review_text'] = reviews_str\n",
        "\n",
        "# agg = df[['name', 'review_text']]\n",
        "\n",
        "# tagged_train_docs = [[TaggedDocument(words=c, tags=[d]) for d, c in agg[['name', 'review_text']].values]]\n",
        "# tagged_train_docs\n",
        "\n",
        "# # doc_vectorizer.build_vocab(tagged_train_docs)\n",
        "\n",
        "# # print(str(doc_vectorizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3t4juWxpJoV"
      },
      "source": [
        "## Model Selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFstSCqeLk7G"
      },
      "outputs": [],
      "source": [
        "# from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# # Create a tokenizer\n",
        "# tokenizer = Tokenizer()\n",
        "\n",
        "# # Fit the tokenizer on the list of review tokens\n",
        "# tokenizer.fit_on_texts(df['review_tokens'])\n",
        "\n",
        "# # Convert the list of tokenized reviews to sequences\n",
        "# sequences = tokenizer.texts_to_sequences(df['review_tokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAr7tc_S6LC2"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# import nltk\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# import gc\n",
        "\n",
        "# nltk.download('vader_lexicon')\n",
        "\n",
        "# # Initialize the VADER sentiment analyzer\n",
        "# sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# # Define a function to convert each review to a sequence of vectors\n",
        "# def reviews_to_vectors_with_sentiment(reviews, tokenizer, max_length, sid, embedding_model):\n",
        "#     # Convert the tokenized reviews to sequences of word indices\n",
        "#     sequences = tokenizer.texts_to_sequences(reviews)\n",
        "\n",
        "#     # Convert the sequences of word indices to sequences of word embeddings\n",
        "#     vectors = []\n",
        "#     for seq in sequences:\n",
        "#         vector_seq = []\n",
        "#         for word_index in seq:\n",
        "#             word = tokenizer.index_word[word_index]\n",
        "#             if word in embedding_model:\n",
        "#                 vector_seq.append(embedding_model[word])\n",
        "#         vectors.append(vector_seq)\n",
        "\n",
        "#     # Pad the sequences with zeros so that all reviews have the same length\n",
        "#     padded_vectors = pad_sequences(vectors, maxlen=max_length, padding='post')\n",
        "\n",
        "#     # Calculate sentiment scores for each review\n",
        "#     sentiment_scores = []\n",
        "#     for i, review in enumerate(reviews):\n",
        "#         ss = sid.polarity_scores(review)\n",
        "#         sentiment_scores.append(ss['compound'])\n",
        "#         if i % 1000 == 0:\n",
        "#             gc.collect()  # Call the garbage collector every 1000 reviews to free up memory\n",
        "    \n",
        "#     sentiment_scores = np.array(sentiment_scores)\n",
        "#     return padded_vectors, sentiment_scores\n",
        "\n",
        "# # Convert the training and testing data to sequences of vectors\n",
        "# max_length = 100  # Set the maximum length of a review to 100 words\n",
        "# train_vectors, train_sentiment = reviews_to_vectors_with_sentiment(train_data['text'], tokenizer, max_length, sid, word_vectors)\n",
        "# test_vectors, test_sentiment = reviews_to_vectors_with_sentiment(test_data['text'], tokenizer, max_length, sid, word_vectors)\n",
        "\n",
        "# # Split the data into input and output arrays\n",
        "# X_train = train_vectors\n",
        "# y_train = train_sentiment\n",
        "# X_test = test_vectors\n",
        "# y_test = test_sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrSQsPKwbH_w"
      },
      "source": [
        "I can use unsupervised sentiment analysis techniques such as sentiment lexicons to estimate the sentiment of your reviews. A sentiment lexicon is a collection of words and their associated sentiment scores (e.g., positive, negative, or neutral).\n",
        "\n",
        "The sentiment_scores list returned by the reviews_to_vectors_with_sentiment function is a list of dictionaries, which cannot be directly encoded by the LabelEncoder or any other encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RWDESRHMa0s"
      },
      "outputs": [],
      "source": [
        "# # Build deep learning model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=num_words, output_dim=100, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
        "# model.add(LSTM(64))\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Train model on training data\n",
        "# model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# # Evaluate model on test data\n",
        "# score = model.evaluate(X_test_padded, y_test, batch_size=32)\n",
        "\n",
        "# # Make restaurant recommendations\n",
        "# user_preferences = 'I want a cheap Italian restaurant near downtown'\n",
        "# user_preferences = ' '.join([stemmer.stem(token) for token in tokenizer(user_preferences.lower())])\n",
        "# user_sequence = tokenizer.texts_to_sequences([user_preferences])\n",
        "# user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length)\n",
        "# predicted_ratings = model.predict(user_padded)\n",
        "# top_restaurants = df.loc[df['stars'].isin(np.argsort(predicted_ratings)[-10:])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKbqKiOvjCQU"
      },
      "outputs": [],
      "source": [
        "# print(train_vectors.shape)\n",
        "# print(y_train.shape)\n",
        "# print(train_vectors.dtype)\n",
        "# print(y_train.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOWT37Px9jBp"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "I will train the deep neural network using the training set and validate it using the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKVN9-kt80FG"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "# from tensorflow.keras.models import Sequential\n",
        "\n",
        "# # Define the RNN model architecture\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=len(word2vec_model.wv.vocab), output_dim=100, weights=[tfidf], input_length=max_length))\n",
        "# model.add(SimpleRNN(units=128))\n",
        "# model.add(Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIr8Zj2d847k"
      },
      "source": [
        "This code defines a simple RNN model with an Embedding layer, a SimpleRNN layer, and a Dense output layer. The Embedding layer is initialized with the pre-trained word vectors, and the weights are fixed during training. The SimpleRNN layer processes the sequence of word vectors and produces a final output, which is passed through a Dense layer with a sigmoid activation function to produce a binary classification output.\n",
        "\n",
        "I chose the RNN model with an Embedding layer, a SimpleRNN layer, and a Dense output layer because it is a popular and effective model for processing sequential data like text. The Embedding layer is used to convert the sequence of words represented by the pre-trained word embeddings into a sequence of dense vectors that can be understood by the RNN model. The SimpleRNN layer is used to process the sequence of dense vectors and capture the relationships between the words in the sequence. Finally, the Dense output layer is used to predict the sentiment of the restaurant review based on the relationships between the words captured by the RNN model.\n",
        "\n",
        "The RNN model is a good choice for this problem because it can capture the contextual relationships between words in a sentence. It is well-suited for text classification tasks like sentiment analysis, where the order of words in a sentence is important for determining the sentiment of the sentence. Additionally, the SimpleRNN layer is a computationally efficient choice for this problem, as it is capable of capturing short-term dependencies in the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9MHVn4-9j_Z"
      },
      "outputs": [],
      "source": [
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(train_vectors, train_labels, epochs=10, batch_size=64, validation_data=(test_vectors, test_labels))\n",
        "\n",
        "# # Evaluate the model\n",
        "# loss, accuracy = model.evaluate(test_vectors, test_labels, verbose=False)\n",
        "# print(f'Test accuracy: {accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiLj7aHV9mB4"
      },
      "source": [
        "The model is then compiled with an Adam optimizer and binary cross-entropy loss function.\n",
        "\n",
        "For binary classification problems, where the goal is to classify an input into one of two classes, binary cross-entropy is a common loss function used in neural networks. It calculates the difference between the predicted probabilities and the true class labels and updates the model weights accordingly. It is often chosen because it has desirable mathematical properties and has been shown to work well in practice.\n",
        "\n",
        "As for the metrics, accuracy is a commonly used metric for classification problems. It measures the proportion of correctly classified examples out of all the examples. In this case, it tells us the percentage of reviews that are correctly classified as positive or negative. By using accuracy as the metric, we can easily evaluate the performance of the model and compare it to other models that use the same metric.\n",
        "\n",
        "It is trained on the train_vectors and train_labels arrays for 10 epochs with a batch size of 64. Finally, the model is evaluated on the test_vectors and test_labels arrays, and the test accuracy is printed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}